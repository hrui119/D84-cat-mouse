CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

________________________________________________

Student Name 1 (last, first): Rui, Haowen

Student Name 2 (last, first): Alexander, Andreas

Student number 1: 1005843688 

Student number 2: 1005800368

UTORid 1: ruihaowe

UTORid 2: alexa376

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _Haowen Rui__	_Andreas Alexander__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
      
      We took into account the cat distance, cheese distance, number
      of possible moves, and how "centered" is the mouse. The cat distance
      gives incentive for the mouse to avoid the cat, the cheese distance 
      is for the mouse to actually try to get the cheese (which is the 
      point of the game). The number of possible moves increases the
      chance of the mouse to escape the cat abd eat the cheese, and the
      "center"ness factor also tells a little bit of how good the game 
      state is because getting cornered will result in the mouse getting 
      eaten to be more likely, and same thing with more possible moves.

2 .- These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.064547

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 0.701793 (average success rate)

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 0.064525

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 0.870055 (average success rate)

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?    
     
     It will only keep improving to some point of convergence, where it cannot get
     any better than that for our particular implementation.  

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate: 0.490710 (avg success rate)

     (1 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate: 0.550624 (avg success rate) 

     Average rate for Experiement 3 and Experiment 4: 0.520667 

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
     
     We are using the QTable from the training done in experiment 2, from a different 
     random seed. These results show that the QTable that we get at the end of each 
     training is mostly useful to the particular seed we got it from, and not so well
     on other seeds. 

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts): 0.030324

     Mouse Winning Rate (from evaluation after training): 0.721532

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
     
     It is not as good as the result from Experiment 2, since with a larger state space,
     there would be more to explore, and with the same training method, it would be
     expected to get a worse result compared to a smaller state space. There would be 
     less exploration done and therefore less learning done as well, which is reflected
     in the worse success rate from the result we got.

6 .- (2 marks) Is standard Q-Learning a rasonable strategy for environments
     that change constantly? discuss based on the above
     
     It is not. As we see from the various experiments, a change in the random seed
     would cause a particular result from another seed to not perform as well. So a change
     in the environment is not suitable for standard Q-learning, as we would need to 
     restart the learning process for a particular environment before we can get a good result.
     When the map changes (random seed) or the state space grew larger, the QTable we got from
     a different environment would not do as well, so Q-Learning is not a reasonable strategy
     for constantly changing environments.

7 .- (2 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
     
     Mouse Winning Rate (from evaluation after training):

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training):

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?
     
9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts):
     
     Mouse Winning Rate (from evaluation after training):
  
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?
      
      
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn 				x
 update

Reward				x
 function

Decide				x
 action

featureEval			x

evaluateQsa			x

maxQsa_prime			x

Qlearn_features			x
	
decideAction_features		x

_____________________________________________________

Marking:

(5 marks) Implemented QLearn update.

(3 marks) Implemented a reasonable reward function

(3 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(10 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(5 marks) Implemented a working feature-based Q-learning
	   algorithm

(10 marks) Competitive mouse performance

(24 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 60

